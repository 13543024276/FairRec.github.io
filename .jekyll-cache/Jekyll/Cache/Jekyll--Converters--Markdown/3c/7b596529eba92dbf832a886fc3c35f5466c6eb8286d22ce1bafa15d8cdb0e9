I"“<!-- - [<font color="DarkOrchid">' </font>\|<font color="DarkSalmon"> </font>]() \| <font color="Violet">[ Fairness +  Fairness]</font> \| [<font color="DeepPink">çŸ¥ä¹Ž</font>]() -->
<ul>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3485447.3511958?casa_token=ci1zK1MeljkAAAAA:4f35VBxLz2p0up57OIDjNYwbrQWTaYWjIm5k3gFPpfIJryXu4yrPEf0X23ppAGbf4BlnMsIOfSH90JU"><font color="DarkOrchid">WWW'22 </font>|<font color="DarkSalmon"> FairGAN: GANs-based Fairness-aware Learning for Recommendations with Implicit Feedback</font></a> | <font color="Violet">[Individual Fairness + Item Fairness]</font> | <a href="https://zhuanlan.zhihu.com/p/508898118"><font color="DeepPink">çŸ¥ä¹Ž</font></a></li>
  <li><a href="https://dl.acm.org/doi/10.1145/3534678.3539269"><font color="DarkOrchid">KDD'22 </font>|<font color="DarkSalmon"> Comprehensive Fair Meta-learned Recommender System</font></a> | <font color="Violet">[Individual Fairness + Group Fairness + User Fairness]</font> | <a href="https://zhuanlan.zhihu.com/p/554268676"><font color="DeepPink">çŸ¥ä¹Ž</font></a></li>
  <li><a href="https://arxiv.org/abs/2204.00541"><font color="DarkOrchid">Arxiv'22 </font>|<font color="DarkSalmon"> FairRank: Fairness-aware Single-tower Ranking Framework for News Recommendation</font></a> | <font color="Violet">[Individual Fairness + User Fairness]</font></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16573"><font color="DarkOrchid">AAAI'21 </font>|<font color="DarkSalmon"> Fairness-aware News Recommendation with Decomposed Adversarial Learning</font></a> | <font color="Violet">[Individual Fairness + User Fairness]</font> | <a href="https://zhuanlan.zhihu.com/p/437339355"><font color="DeepPink">çŸ¥ä¹Ž</font></a></li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3404835.3462966?casa_token=IKPz455rovAAAAAA:YkffjU_9A2HfKXmgYBGWIcQnpPqBVhE8z8yzxCnu63rnb2PPFLnEYt9x4N9JSrcl0h6SggYmy4mA7YQ"><font color="DarkOrchid">SIGIR'21 </font>|<font color="DarkSalmon"> Towards Personalized Fairness based on Causal Notion</font></a> | <font color="Violet">[Individual Fairness + User Fairness]</font> | <a href="https://zhuanlan.zhihu.com/p/432969760"><font color="DeepPink">çŸ¥ä¹Ž</font></a></li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3442381.3450015?casa_token=JZP89APevqoAAAAA:x4adSwi12IUwGVEcolVAaAoAjRkfPiIwkeHu8QvhHYK9qhHOIUYQsFsv4BTLq9x5qZpXoon6i_GztDs"><font color="DarkOrchid">WWW'21 </font>|<font color="DarkSalmon"> Learning Fair Representations for Recommendation: A Graph-based Perspective</font></a> | <font color="Violet">[Individual Fairness + User Fairness]</font> | <a href="https://zhuanlan.zhihu.com/p/433648567"><font color="DeepPink">çŸ¥ä¹Ž</font></a></li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3437963.3441752"><font color="DarkOrchid">WSDM'21 </font>|<font color="DarkSalmon"> Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information</font></a> | <font color="Violet">[Individual Fairness + User Fairness]</font> | <a href="https://zhuanlan.zhihu.com/p/554125217"><font color="DeepPink">çŸ¥ä¹Ž</font></a></li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401177"><font color="DarkOrchid">SIGIR'20 </font>|<font color="DarkSalmon"> Measuring and Mitigating Item Under-Recommendation Bias in Personalized Ranking Systems</font></a> | <font color="Violet">[Group Fairness + Item Fairness]</font> | <a href="https://zhuanlan.zhihu.com/p/477799550"><font color="DeepPink">çŸ¥ä¹Ž</font></a></li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3336191.3371832?casa_token=T6LMYf-kzXoAAAAA:oPhyhlRNpegcJ4sIF4kGcmQtfeFaZa0oQcWo3TajY3yHimCtGLVWH9GFSy--7bOGUUPFQSoINGEeGXE"><font color="DarkOrchid">WSDM'20 </font>|<font color="DarkSalmon"> Privacy-Aware Recommendation with Private-Attribute Protection using Adversarial Learning</font></a> | <font color="Violet">[Individual Fairness + User Fairness]</font></li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401177"><font color="DarkOrchid">PMLR'19 </font>|<font color="DarkSalmon"> Compositional Fairness Constraints for Graph Embeddings</font></a> | <font color="Violet">[Individual Fairness + User Fairness]</font> | <a href="https://zhuanlan.zhihu.com/p/472853603"><font color="DeepPink">çŸ¥ä¹Ž</font></a></li>
</ul>

<h1 id="other-fields">Other Fields</h1>

<ul>
  <li><a href="https://arxiv.org/abs/1511.05897"><font color="DarkOrchid">ICLR'15 </font>|<font color="DarkSalmon"> Censoring Representations with an Adversary</font></a></li>
</ul>

<p>Censoring representations with an adversary
<!-- Adversarial Learning for Debiasing Knowledge Graph Embeddings -->
<!-- Data decisions and theoretical implications when adversarially learning fair representations --></p>
:ET